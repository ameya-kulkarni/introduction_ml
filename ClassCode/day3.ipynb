{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### arrays, one d, 2 d arrays (matrices vectors)\n",
    "#### tensors ##### accelerated over a gpu\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2,3]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1,2,3]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2,3]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros([1,2,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2,3]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu') #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.]]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros([1,2,2,3]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.],\n",
       "         [0., 0., 0.]]], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Gradient descent (gradient matrix) \n",
    "torch.zeros([1,2,3],requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minmize $X^2+4X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor(0.0,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=x*x+4*x ### forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad ### Grad of x at 0 wrt z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=x*x+4*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad ### Start accumulating the value of gradients,\n",
    "### Pass 1\n",
    "### grad=4\n",
    "### Pass 2\n",
    "### grad_now+grad_prev=4+4=8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Forward pass\n",
    "#### backward()\n",
    "#### zero out the graidients\n",
    "### xnew=xold-lr*(sum(grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z 0.0, x: -0.03999999910593033\n",
      "Z -0.15839999914169312, x: -0.07919999957084656\n",
      "Z -0.31052735447883606, x: -0.11761599779129028\n",
      "Z -0.4566304683685303, x: -0.15526367723941803\n",
      "Z -0.5969479084014893, x: -0.19215840101242065\n",
      "Z -0.7317087650299072, x: -0.22831523418426514\n",
      "Z -0.8611330986022949, x: -0.2637489140033722\n",
      "Z -0.9854321479797363, x: -0.29847392439842224\n",
      "Z -1.104809045791626, x: -0.3325044512748718\n",
      "Z -1.2194585800170898, x: -0.3658543527126312\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor(0.0,requires_grad=True)\n",
    "lr=0.01\n",
    "for i in range(10):\n",
    "    z=x*x+4*x\n",
    "    z.backward() ### dz/dx\n",
    "    with torch.no_grad(): ##Disables any gradient computation\n",
    "        x-=lr*x.grad\n",
    "        x.grad.zero_()\n",
    "    print(f\"Z {z}, x: {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Autodiff\n",
    "### xy=750=>x=750/y\n",
    "## x+10y Minimize this\n",
    "### 750/y+y*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z: 760.0, x: 8.399999618530273\n",
      "Z: 173.2857208251953, x: 8.406291961669922\n",
      "Z: 173.28179931640625, x: 8.41242504119873\n",
      "Z: 173.27809143066406, x: 8.418403625488281\n",
      "Z: 173.27456665039062, x: 8.42423152923584\n",
      "Z: 173.27120971679688, x: 8.429913520812988\n",
      "Z: 173.2680206298828, x: 8.435453414916992\n",
      "Z: 173.26498413085938, x: 8.4408540725708\n",
      "Z: 173.26211547851562, x: 8.446120262145996\n",
      "Z: 173.25936889648438, x: 8.451254844665527\n",
      "Z: 173.25677490234375, x: 8.45626163482666\n",
      "Z: 173.25428771972656, x: 8.46114444732666\n",
      "Z: 173.25193786621094, x: 8.465906143188477\n",
      "Z: 173.24969482421875, x: 8.470550537109375\n",
      "Z: 173.24755859375, x: 8.475079536437988\n",
      "Z: 173.24554443359375, x: 8.479496955871582\n",
      "Z: 173.2436065673828, x: 8.483805656433105\n",
      "Z: 173.2417755126953, x: 8.488008499145508\n",
      "Z: 173.24002075195312, x: 8.492108345031738\n",
      "Z: 173.23837280273438, x: 8.496108055114746\n",
      "Z: 173.23678588867188, x: 8.500009536743164\n",
      "Z: 173.23529052734375, x: 8.503815650939941\n",
      "Z: 173.23385620117188, x: 8.507528305053711\n",
      "Z: 173.2324981689453, x: 8.511151313781738\n",
      "Z: 173.231201171875, x: 8.51468563079834\n",
      "Z: 173.22998046875, x: 8.518134117126465\n",
      "Z: 173.22879028320312, x: 8.521498680114746\n",
      "Z: 173.22767639160156, x: 8.524782180786133\n",
      "Z: 173.22659301757812, x: 8.527985572814941\n",
      "Z: 173.2255859375, x: 8.531111717224121\n",
      "Z: 173.22463989257812, x: 8.534162521362305\n",
      "Z: 173.22372436523438, x: 8.537138938903809\n",
      "Z: 173.22283935546875, x: 8.540043830871582\n",
      "Z: 173.2220001220703, x: 8.542879104614258\n",
      "Z: 173.22120666503906, x: 8.545645713806152\n",
      "Z: 173.220458984375, x: 8.548345565795898\n",
      "Z: 173.2197265625, x: 8.550980567932129\n",
      "Z: 173.21905517578125, x: 8.553552627563477\n",
      "Z: 173.2183837890625, x: 8.556062698364258\n",
      "Z: 173.2177734375, x: 8.558512687683105\n",
      "Z: 173.21717834472656, x: 8.560904502868652\n",
      "Z: 173.21661376953125, x: 8.563239097595215\n",
      "Z: 173.216064453125, x: 8.56551742553711\n",
      "Z: 173.21556091308594, x: 8.567741394042969\n",
      "Z: 173.21507263183594, x: 8.569912910461426\n",
      "Z: 173.214599609375, x: 8.57203197479248\n",
      "Z: 173.21417236328125, x: 8.574100494384766\n",
      "Z: 173.2137451171875, x: 8.576120376586914\n",
      "Z: 173.21331787109375, x: 8.578091621398926\n",
      "Z: 173.21295166015625, x: 8.580016136169434\n",
      "Z: 173.21258544921875, x: 8.581894874572754\n",
      "Z: 173.2122344970703, x: 8.58372974395752\n",
      "Z: 173.2119140625, x: 8.58552074432373\n",
      "Z: 173.21157836914062, x: 8.587268829345703\n",
      "Z: 173.21127319335938, x: 8.58897590637207\n",
      "Z: 173.21099853515625, x: 8.590642929077148\n",
      "Z: 173.21072387695312, x: 8.592269897460938\n",
      "Z: 173.21044921875, x: 8.59385871887207\n",
      "Z: 173.210205078125, x: 8.595409393310547\n",
      "Z: 173.20997619628906, x: 8.596923828125\n",
      "Z: 173.20974731445312, x: 8.598402976989746\n",
      "Z: 173.20953369140625, x: 8.599846839904785\n",
      "Z: 173.20932006835938, x: 8.601256370544434\n",
      "Z: 173.20913696289062, x: 8.602632522583008\n",
      "Z: 173.2089385986328, x: 8.603976249694824\n",
      "Z: 173.20877075195312, x: 8.6052885055542\n",
      "Z: 173.20858764648438, x: 8.60657024383545\n",
      "Z: 173.20843505859375, x: 8.607821464538574\n",
      "Z: 173.20828247070312, x: 8.60904312133789\n",
      "Z: 173.2081298828125, x: 8.610236167907715\n",
      "Z: 173.20799255371094, x: 8.611401557922363\n",
      "Z: 173.20785522460938, x: 8.612539291381836\n",
      "Z: 173.20773315429688, x: 8.61365032196045\n",
      "Z: 173.20761108398438, x: 8.61473560333252\n",
      "Z: 173.20748901367188, x: 8.615795135498047\n",
      "Z: 173.20736694335938, x: 8.616829872131348\n",
      "Z: 173.207275390625, x: 8.617840766906738\n",
      "Z: 173.2071533203125, x: 8.618827819824219\n",
      "Z: 173.2070770263672, x: 8.619791030883789\n",
      "Z: 173.20697021484375, x: 8.620732307434082\n",
      "Z: 173.20687866210938, x: 8.621651649475098\n",
      "Z: 173.20680236816406, x: 8.622549057006836\n",
      "Z: 173.20672607421875, x: 8.623425483703613\n",
      "Z: 173.20664978027344, x: 8.624281883239746\n",
      "Z: 173.20657348632812, x: 8.625118255615234\n",
      "Z: 173.20651245117188, x: 8.625934600830078\n",
      "Z: 173.20645141601562, x: 8.626731872558594\n",
      "Z: 173.20639038085938, x: 8.627511024475098\n",
      "Z: 173.20632934570312, x: 8.628271102905273\n",
      "Z: 173.20626831054688, x: 8.629014015197754\n",
      "Z: 173.20620727539062, x: 8.629739761352539\n",
      "Z: 173.20614624023438, x: 8.630448341369629\n",
      "Z: 173.20611572265625, x: 8.63114070892334\n",
      "Z: 173.2060546875, x: 8.631816864013672\n",
      "Z: 173.20602416992188, x: 8.632476806640625\n",
      "Z: 173.2059783935547, x: 8.633121490478516\n",
      "Z: 173.2059326171875, x: 8.633750915527344\n",
      "Z: 173.20590209960938, x: 8.634366035461426\n",
      "Z: 173.2058563232422, x: 8.634966850280762\n",
      "Z: 173.20582580566406, x: 8.635553359985352\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor(1.0,requires_grad=True)\n",
    "lr=0.01\n",
    "for i in range(100):\n",
    "    z=(750/x)+x*10\n",
    "    z.backward()\n",
    "    with torch.no_grad():\n",
    "        x-=lr*x.grad\n",
    "        x.grad.zero_()\n",
    "    print(f\"Z: {z}, x: {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ML problem: \n",
    "import pandas as pd\n",
    "reg=pd.read_csv(\"../sony/data/regression.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>year</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n",
       "0  18.0        8.0         307.0       130.0  3504.0          12.0  70.0   \n",
       "1  15.0        8.0         350.0       165.0  3693.0          11.5  70.0   \n",
       "\n",
       "   origin  \n",
       "0     1.0  \n",
       "1     1.0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### mpg=b0+b1*cyl ## as a matrix product?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dloss/db0, dloss/db1\n",
    "#### loss=f(b0,b1)\n",
    "##loss=eq\n",
    "##loss.backward()\n",
    "##b0.grad\n",
    "##b1.grad\n",
    "X=reg[['cylinders']].values\n",
    "y=reg[['mpg']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## W dim?=> dloss/dW,dloss/db\n",
    "W=torch.randn(1,1,requires_grad=True)\n",
    "b=torch.randn(1,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### loss?\n",
    "loss=sum((y-XW+b)^2)/n\n",
    "loss.backward()\n",
    "W.grad\n",
    "b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=torch.tensor(X)\n",
    "y=torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 106.57937383717696, W: [[1.5394461]], b: [-12.498031]\n",
      "Loss: 106.29652724282161, W: [[1.5306689]], b: [-12.550415]\n",
      "Loss: 106.01465406982496, W: [[1.5219065]], b: [-12.602709]\n",
      "Loss: 105.73375239029292, W: [[1.5131594]], b: [-12.654913]\n",
      "Loss: 105.4538148533757, W: [[1.5044272]], b: [-12.707027]\n",
      "Loss: 105.17483884401985, W: [[1.4957101]], b: [-12.759052]\n",
      "Loss: 104.8968210069427, W: [[1.4870082]], b: [-12.810987]\n",
      "Loss: 104.61976103636141, W: [[1.4783207]], b: [-12.862833]\n",
      "Loss: 104.34365403355959, W: [[1.4696487]], b: [-12.91459]\n",
      "Loss: 104.06849371735905, W: [[1.4609914]], b: [-12.966257]\n",
      "Loss: 103.79428275126844, W: [[1.452349]], b: [-13.017836]\n",
      "Loss: 103.52101361310103, W: [[1.4437215]], b: [-13.069325]\n",
      "Loss: 103.24868271208203, W: [[1.4351087]], b: [-13.120727]\n",
      "Loss: 102.97728938506957, W: [[1.426511]], b: [-13.17204]\n",
      "Loss: 102.70682585822372, W: [[1.417928]], b: [-13.223265]\n",
      "Loss: 102.43729364242495, W: [[1.4093596]], b: [-13.274402]\n",
      "Loss: 102.16868574302856, W: [[1.4008062]], b: [-13.32545]\n",
      "Loss: 101.9010047640723, W: [[1.3922671]], b: [-13.3764105]\n",
      "Loss: 101.63424392719688, W: [[1.383743]], b: [-13.427283]\n",
      "Loss: 101.36840038466318, W: [[1.3752334]], b: [-13.478069]\n",
      "Loss: 101.10346696857538, W: [[1.3667387]], b: [-13.528768]\n",
      "Loss: 100.83944657207539, W: [[1.3582585]], b: [-13.579379]\n",
      "Loss: 100.57633189425486, W: [[1.3497927]], b: [-13.629903]\n",
      "Loss: 100.31412440868142, W: [[1.3413416]], b: [-13.68034]\n",
      "Loss: 100.05281752603965, W: [[1.332905]], b: [-13.73069]\n",
      "Loss: 99.79240902915474, W: [[1.3244832]], b: [-13.780953]\n",
      "Loss: 99.53289683086567, W: [[1.3160756]], b: [-13.831131]\n",
      "Loss: 99.27427338304174, W: [[1.3076826]], b: [-13.881222]\n",
      "Loss: 99.01654162565809, W: [[1.2993039]], b: [-13.931227]\n",
      "Loss: 98.75969341752857, W: [[1.2909396]], b: [-13.981146]\n",
      "Loss: 98.5037277684382, W: [[1.2825896]], b: [-14.030979]\n",
      "Loss: 98.24864168729665, W: [[1.2742538]], b: [-14.080727]\n",
      "Loss: 97.99443261730507, W: [[1.2659328]], b: [-14.130388]\n",
      "Loss: 97.74109908437582, W: [[1.257626]], b: [-14.179964]\n",
      "Loss: 97.48863842243667, W: [[1.2493335]], b: [-14.229455]\n",
      "Loss: 97.23704367063091, W: [[1.2410551]], b: [-14.278861]\n",
      "Loss: 96.98631226261628, W: [[1.2327911]], b: [-14.328182]\n",
      "Loss: 96.73644225327435, W: [[1.2245412]], b: [-14.3774185]\n",
      "Loss: 96.48743114163213, W: [[1.2163055]], b: [-14.42657]\n",
      "Loss: 96.23927704862936, W: [[1.208084]], b: [-14.4756365]\n",
      "Loss: 95.99197747220049, W: [[1.1998769]], b: [-14.524619]\n",
      "Loss: 95.74552638659085, W: [[1.1916834]], b: [-14.573517]\n",
      "Loss: 95.49992469960344, W: [[1.1835042]], b: [-14.622331]\n",
      "Loss: 95.25516651145746, W: [[1.175339]], b: [-14.671061]\n",
      "Loss: 95.01124919913075, W: [[1.1671882]], b: [-14.719707]\n",
      "Loss: 94.76817127068546, W: [[1.1590513]], b: [-14.768269]\n",
      "Loss: 94.5259298817889, W: [[1.1509284]], b: [-14.816748]\n",
      "Loss: 94.28451859372065, W: [[1.1428194]], b: [-14.865143]\n",
      "Loss: 94.04393987965155, W: [[1.1347246]], b: [-14.913455]\n",
      "Loss: 93.80418681530303, W: [[1.1266433]], b: [-14.961684]\n",
      "Loss: 93.56525705880594, W: [[1.118576]], b: [-15.00983]\n",
      "Loss: 93.32714842416925, W: [[1.1105225]], b: [-15.057894]\n",
      "Loss: 93.08985907273593, W: [[1.102483]], b: [-15.105874]\n",
      "Loss: 92.85338694636683, W: [[1.0944574]], b: [-15.153772]\n",
      "Loss: 92.6177251570165, W: [[1.0864456]], b: [-15.201588]\n",
      "Loss: 92.38287626920909, W: [[1.0784473]], b: [-15.249321]\n",
      "Loss: 92.14883341928909, W: [[1.0704631]], b: [-15.296972]\n",
      "Loss: 91.9155945857501, W: [[1.0624925]], b: [-15.344542]\n",
      "Loss: 91.68315825547626, W: [[1.0545356]], b: [-15.392029]\n",
      "Loss: 91.45152124433464, W: [[1.0465924]], b: [-15.439435]\n",
      "Loss: 91.22067785915773, W: [[1.038663]], b: [-15.486759]\n",
      "Loss: 90.99063023209378, W: [[1.030747]], b: [-15.534002]\n",
      "Loss: 90.76137154011904, W: [[1.0228447]], b: [-15.581163]\n",
      "Loss: 90.53290406672578, W: [[1.0149562]], b: [-15.628243]\n",
      "Loss: 90.30522244473264, W: [[1.0070812]], b: [-15.675243]\n",
      "Loss: 90.07831923122174, W: [[0.99921966]], b: [-15.722162]\n",
      "Loss: 89.85219662331951, W: [[0.99137163]], b: [-15.769]\n",
      "Loss: 89.62685315078838, W: [[0.983537]], b: [-15.815758]\n",
      "Loss: 89.40228213898385, W: [[0.9757159]], b: [-15.862434]\n",
      "Loss: 89.17848586582821, W: [[0.96790826]], b: [-15.909031]\n",
      "Loss: 88.95545795402309, W: [[0.9601142]], b: [-15.955547]\n",
      "Loss: 88.73319685791502, W: [[0.95233357]], b: [-16.001984]\n",
      "Loss: 88.51170043897277, W: [[0.9445662]], b: [-16.04834]\n",
      "Loss: 88.29096583434749, W: [[0.9368121]], b: [-16.094616]\n",
      "Loss: 88.07099186830436, W: [[0.9290715]], b: [-16.140814]\n",
      "Loss: 87.85176787295342, W: [[0.9213443]], b: [-16.186932]\n",
      "Loss: 87.63330025743231, W: [[0.9136301]], b: [-16.23297]\n",
      "Loss: 87.41558726129708, W: [[0.90592915]], b: [-16.278929]\n",
      "Loss: 87.1986177239991, W: [[0.8982418]], b: [-16.324808]\n",
      "Loss: 86.98239927493378, W: [[0.89056754]], b: [-16.37061]\n",
      "Loss: 86.7669205094124, W: [[0.88290644]], b: [-16.416332]\n",
      "Loss: 86.55217989465888, W: [[0.8752587]], b: [-16.461975]\n",
      "Loss: 86.33818422257363, W: [[0.86762387]], b: [-16.50754]\n",
      "Loss: 86.12492258030417, W: [[0.8600023]], b: [-16.553026]\n",
      "Loss: 85.91239314884028, W: [[0.8523936]], b: [-16.598434]\n",
      "Loss: 85.70059406368182, W: [[0.84479827]], b: [-16.643764]\n",
      "Loss: 85.48952348366416, W: [[0.83721596]], b: [-16.689016]\n",
      "Loss: 85.27917971123543, W: [[0.8296469]], b: [-16.734192]\n",
      "Loss: 85.06955201714128, W: [[0.82209045]], b: [-16.77929]\n",
      "Loss: 84.860646820288, W: [[0.81454694]], b: [-16.824308]\n",
      "Loss: 84.65246222574288, W: [[0.80701655]], b: [-16.869251]\n",
      "Loss: 84.44498858129145, W: [[0.7994992]], b: [-16.914116]\n",
      "Loss: 84.23823210859764, W: [[0.7919949]], b: [-16.958902]\n",
      "Loss: 84.03219094634537, W: [[0.7845033]], b: [-17.003613]\n",
      "Loss: 83.82685486704993, W: [[0.7770247]], b: [-17.048246]\n",
      "Loss: 83.62222155257871, W: [[0.7695587]], b: [-17.092804]\n",
      "Loss: 83.41828970460764, W: [[0.76210576]], b: [-17.137283]\n",
      "Loss: 83.21506570844782, W: [[0.75466555]], b: [-17.181686]\n",
      "Loss: 83.01253908716674, W: [[0.7472385]], b: [-17.226013]\n",
      "Loss: 82.81070862636678, W: [[0.73982394]], b: [-17.270264]\n"
     ]
    }
   ],
   "source": [
    "lr=0.01\n",
    "for i in range(100):\n",
    "    diff=y-torch.matmul(X.float(),W)+b\n",
    "    loss=sum(diff*diff)/y.shape[0]\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        W-=lr*W.grad\n",
    "        b-=lr*b.grad\n",
    "        W.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "    print(f\"Loss: {loss.item()}, W: {W.detach().numpy()}, b: {b.detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### linear classifier.\n",
    "### Can you estimate a linear classifier using autodiff\n",
    "### Loss for linear classifier?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
